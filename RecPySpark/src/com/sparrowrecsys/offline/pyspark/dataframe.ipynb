{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2a292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.2.0\n",
      "hello spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import os\n",
    "spark_home = r\"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\"\n",
    "python_path = r\"C:\\Users\\fywud\\anaconda3\\envs\\pyspark_python\\python\"\n",
    "findspark.init(spark_home,python_path)\n",
    "import pyspark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "print(\"spark version:\",pyspark.__version__)\n",
    "rdd = sc.parallelize([\"hello\",\"spark\"])\n",
    "print(rdd.reduce(lambda x,y:x+' '+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "735c0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|     name|age|score|\n",
      "+---------+---+-----+\n",
      "|    LiLei| 15|   88|\n",
      "|HanMeiMei| 16|   90|\n",
      "|   DaChui| 17|   60|\n",
      "+---------+---+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#将RDD转换成DataFrame\n",
    "rdd = sc.parallelize([(\"LiLei\",15,88),(\"HanMeiMei\",16,90),(\"DaChui\",17,60)])\n",
    "df = rdd.toDF([\"name\",\"age\",\"score\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309466a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|    LiLei| 18|\n",
      "|HanMeiMei| 17|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pdf = pd.DataFrame([(\"LiLei\",18),(\"HanMeiMei\",17)],columns = [\"name\",\"age\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968da749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|    LiLei| 18|\n",
      "|HanMeiMei| 17|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 也可以对列表直接转换\n",
    "values = [(\"LiLei\",18),(\"HanMeiMei\",17)]\n",
    "df = spark.createDataFrame(values,[\"name\",\"age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fdd4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+\n",
      "|     name|score|  birthday|\n",
      "+---------+-----+----------+\n",
      "|    LiLei|   87|2010-01-05|\n",
      "|HanMeiMei|   90|2009-03-01|\n",
      "|   DaChui| null|2008-07-02|\n",
      "+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "schema = StructType([StructField(\"name\", StringType(), nullable = False),\n",
    "                     StructField(\"score\", IntegerType(), nullable = True),\n",
    "                     StructField(\"birthday\", DateType(), nullable = True)])\n",
    "\n",
    "rdd = sc.parallelize([Row(\"LiLei\",87,datetime(2010,1,5)),\n",
    "                      Row(\"HanMeiMei\",90,datetime(2009,3,1)),\n",
    "                      Row(\"DaChui\",None,datetime(2008,7,2))])\n",
    "\n",
    "dfstudent = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "dfstudent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dacfa867",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\fywud\\AppData\\Local\\Temp/ipykernel_9340/444142719.py\", line 8, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9340/444142719.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdfscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfstudents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"scores\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdfmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"udf_mode(scores) as mode_score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdfmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\fywud\\AppData\\Local\\Temp/ipykernel_9340/444142719.py\", line 8, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n"
     ]
    }
   ],
   "source": [
    "students = [(\"class1\",15),(\"class1\",15),(\"class2\",16),(\"class2\",16),(\"class1\",17),(\"class2\",19)]\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "def mode(arr):\n",
    "    dict_cnt = {}\n",
    "    for x in arr:\n",
    "        dict_cnt[x] = dict_cnt.get(x,0)+1\n",
    "    max_cnt = max(dict_cnt.values())\n",
    "    most_values = [k for k,v in dict_cnt.items() if v==max_cnt]\n",
    "    s = 0.0\n",
    "    for x in most_values:\n",
    "        s = s + x\n",
    "    return s/len(most_values)\n",
    "spark.udf.register(\"udf_mode\",mode)\n",
    "dfstudents = spark.createDataFrame(students).toDF(\"class\",\"score\")\n",
    "dfscores = dfstudents.groupBy(\"class\").agg(F.collect_list(\"score\").alias(\"scores\"))\n",
    "dfmode = dfscores.selectExpr(\"class\",\"udf_mode(scores) as mode_score\")\n",
    "dfmode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2198f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
