{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60028a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90693d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = r\"C:\\Program Files\\spark-3.0.1-bin-hadoop3.2\"\n",
    "python_path = r\"C:\\Users\\fywud\\anaconda3\\python\"\n",
    "findspark.init(spark_home,python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfea71cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.0.1\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (DESKTOP-SCD57NT executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 601, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in read_command\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Program Files\\\\spark-3.2.0-bin-hadoop3.2\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 601, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in read_command\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Program Files\\\\spark-3.2.0-bin-hadoop3.2\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-94f50aecdf1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark version:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hello\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"spark\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\spark-3.0.1-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.0.1-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.0.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.0.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (DESKTOP-SCD57NT executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 601, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in read_command\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Program Files\\\\spark-3.2.0-bin-hadoop3.2\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 601, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in read_command\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from 'C:\\\\Program Files\\\\spark-3.2.0-bin-hadoop3.2\\\\python\\\\lib\\\\pyspark.zip\\\\pyspark\\\\cloudpickle\\\\__init__.py'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"test1\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"spark version:\",pyspark.__version__)\n",
    "rdd = sc.parallelize([\"hello\",\"spark\"])\n",
    "print(rdd.reduce(lambda x,y:x+' '+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23489d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark \n",
    "# from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "# # spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b93d8dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-38-da7771c5dbf9>\", line 8, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-da7771c5dbf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdfscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfstudents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"scores\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdfmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"udf_mode(scores) as mode_score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdfmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-38-da7771c5dbf9>\", line 8, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n"
     ]
    }
   ],
   "source": [
    "students = [(\"class1\",15),(\"class1\",15),(\"class2\",16),(\"class2\",16),(\"class1\",17),(\"class2\",19)]\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "def mode(arr):\n",
    "    dict_cnt = {}\n",
    "    for x in arr:\n",
    "        dict_cnt[x] = dict_cnt.get(x,0)+1\n",
    "    max_cnt = max(dict_cnt.values())\n",
    "    most_values = [k for k,v in dict_cnt.items() if v==max_cnt]\n",
    "    s = 0.0\n",
    "    for x in most_values:\n",
    "        s = s + x\n",
    "    return s/len(most_values)\n",
    "spark.udf.register(\"udf_mode\",mode)\n",
    "dfstudents = spark.createDataFrame(students).toDF(\"class\",\"score\")\n",
    "dfscores = dfstudents.groupBy(\"class\").agg(F.collect_list(\"score\").alias(\"scores\"))\n",
    "dfmode = dfscores.selectExpr(\"class\",\"udf_mode(scores) as mode_score\")\n",
    "dfmode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbc99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Working\\GitHub\\SparrowRecSys\\src\\main\\resources'\n",
    "rawSampleDataPath = file_path + \"/webroot/sampledata/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfa5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UdfFunction:\n",
    "    @staticmethod\n",
    "    def sortF(movie_list, timestamp_list):\n",
    "        \"\"\"\n",
    "        sort by time and return the corresponding movie sequence\n",
    "        eg:\n",
    "            input: movie_list:[1,2,3]\n",
    "                   timestamp_list:[1112486027,1212546032,1012486033]\n",
    "            return [3,1,2]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for m, t in zip(movie_list, timestamp_list):\n",
    "            pairs.append((m, t))\n",
    "        # sort by time\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])\n",
    "        return [x[0] for x in pairs]\n",
    "    \n",
    "\n",
    "def processItemSequence(spark, rawSampleDataPath):\n",
    "    # rating data\n",
    "    ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "    # ratingSamples.show(5)\n",
    "    # ratingSamples.printSchema()\n",
    "    sortUdf = udf(UdfFunction.sortF, ArrayType(StringType()))\n",
    "    userSeq = ratingSamples \\\n",
    "        .where(F.col(\"rating\") >= 3.5) \\\n",
    "        .groupBy(\"userId\") \\\n",
    "        .agg(sortUdf(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias('movieIds')) \\\n",
    "        .withColumn(\"movieIdStr\", array_join(F.col(\"movieIds\"), \" \"))\n",
    "    # userSeq.select(\"userId\", \"movieIdStr\").show(10, truncate = False)\n",
    "    return userSeq.select('movieIdStr').rdd.map(lambda x: x[0].split(' '))\n",
    "\n",
    "\n",
    "def embeddingLSH(spark, movieEmbMap):\n",
    "    movieEmbSeq = []\n",
    "    for key, embedding_list in movieEmbMap.items():\n",
    "        embedding_list = [np.float64(embedding) for embedding in embedding_list]\n",
    "        movieEmbSeq.append((key, Vectors.dense(embedding_list)))\n",
    "    movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\")\n",
    "    bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1,\n",
    "                                                      numHashTables=3)\n",
    "    bucketModel = bucketProjectionLSH.fit(movieEmbDF)\n",
    "    embBucketResult = bucketModel.transform(movieEmbDF)\n",
    "    print(\"movieId, emb, bucketId schema:\")\n",
    "    embBucketResult.printSchema()\n",
    "    print(\"movieId, emb, bucketId data result:\")\n",
    "    embBucketResult.show(10, truncate=False)\n",
    "    print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\")\n",
    "    sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237)\n",
    "    bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)\n",
    "\n",
    "\n",
    "def trainItem2vec(spark, samples, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "    word2vec = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "    model = word2vec.fit(samples)\n",
    "    synonyms = model.findSynonyms(\"158\", 20)\n",
    "    for synonym, cosineSimilarity in synonyms:\n",
    "        print(synonym, cosineSimilarity)\n",
    "    embOutputDir = '/'.join(embOutputPath.split('/')[:-1])\n",
    "    if not os.path.exists(embOutputDir):\n",
    "        os.makedirs(embOutputDir)\n",
    "    with open(embOutputPath, 'w') as f:\n",
    "        for movie_id in model.getVectors():\n",
    "            vectors = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "            f.write(movie_id + \":\" + vectors + \"\\n\")\n",
    "    embeddingLSH(spark, model.getVectors())\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_pair(x):\n",
    "    # eg:\n",
    "    # watch sequence:['858', '50', '593', '457']\n",
    "    # return:[['858', '50'],['50', '593'],['593', '457']]\n",
    "    pairSeq = []\n",
    "    previousItem = ''\n",
    "    for item in x:\n",
    "        if not previousItem:\n",
    "            previousItem = item\n",
    "        else:\n",
    "            pairSeq.append((previousItem, item))\n",
    "            previousItem = item\n",
    "    return pairSeq\n",
    "\n",
    "\n",
    "def generateTransitionMatrix(samples):\n",
    "    pairSamples = samples.flatMap(lambda x: generate_pair(x))\n",
    "    pairCountMap = pairSamples.countByValue()\n",
    "    pairTotalCount = 0\n",
    "    transitionCountMatrix = defaultdict(dict)\n",
    "    itemCountMap = defaultdict(int)\n",
    "    for key, cnt in pairCountMap.items():\n",
    "        key1, key2 = key\n",
    "        transitionCountMatrix[key1][key2] = cnt\n",
    "        itemCountMap[key1] += cnt\n",
    "        pairTotalCount += cnt\n",
    "    transitionMatrix = defaultdict(dict)\n",
    "    itemDistribution = defaultdict(dict)\n",
    "    for key1, transitionMap in transitionCountMatrix.items():\n",
    "        for key2, cnt in transitionMap.items():\n",
    "            transitionMatrix[key1][key2] = transitionCountMatrix[key1][key2] / itemCountMap[key1]\n",
    "    for itemid, cnt in itemCountMap.items():\n",
    "        itemDistribution[itemid] = cnt / pairTotalCount\n",
    "    return transitionMatrix, itemDistribution\n",
    "\n",
    "\n",
    "def oneRandomWalk(transitionMatrix, itemDistribution, sampleLength):\n",
    "    sample = []\n",
    "    # pick the first element\n",
    "    randomDouble = random.random()\n",
    "    firstItem = \"\"\n",
    "    accumulateProb = 0.0\n",
    "    for item, prob in itemDistribution.items():\n",
    "        accumulateProb += prob\n",
    "        if accumulateProb >= randomDouble:\n",
    "            firstItem = item\n",
    "            break\n",
    "    sample.append(firstItem)\n",
    "    curElement = firstItem\n",
    "    i = 1\n",
    "    while i < sampleLength:\n",
    "        if (curElement not in itemDistribution) or (curElement not in transitionMatrix):\n",
    "            break\n",
    "        probDistribution = transitionMatrix[curElement]\n",
    "        randomDouble = random.random()\n",
    "        accumulateProb = 0.0\n",
    "        for item, prob in probDistribution.items():\n",
    "            accumulateProb += prob\n",
    "            if accumulateProb >= randomDouble:\n",
    "                curElement = item\n",
    "                break\n",
    "        sample.append(curElement)\n",
    "        i += 1\n",
    "    return sample\n",
    "\n",
    "\n",
    "def randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength):\n",
    "    samples = []\n",
    "    for i in range(sampleCount):\n",
    "        samples.append(oneRandomWalk(transitionMatrix, itemDistribution, sampleLength))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def graphEmb(samples, spark, embLength, embOutputFilename, saveToRedis, redisKeyPrefix):\n",
    "    transitionMatrix, itemDistribution = generateTransitionMatrix(samples)\n",
    "    sampleCount = 20000\n",
    "    sampleLength = 10\n",
    "    newSamples = randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength)\n",
    "    rddSamples = spark.sparkContext.parallelize(newSamples)\n",
    "    trainItem2vec(spark, rddSamples, embLength, embOutputFilename, saveToRedis, redisKeyPrefix)\n",
    "\n",
    "\n",
    "def generateUserEmb(spark, rawSampleDataPath, model, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "    ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "    Vectors_list = []\n",
    "    for key, value in model.getVectors().items():\n",
    "        Vectors_list.append((key, list(value)))\n",
    "    fields = [\n",
    "        StructField('movieId', StringType(), False),\n",
    "        StructField('emb', ArrayType(FloatType()), False)\n",
    "    ]\n",
    "    schema = StructType(fields)\n",
    "    Vectors_df = spark.createDataFrame(Vectors_list, schema=schema)\n",
    "    ratingSamples = ratingSamples.join(Vectors_df, on='movieId', how='inner')\n",
    "    result = ratingSamples.select('userId', 'emb').rdd.map(lambda x: (x[0], x[1])) \\\n",
    "        .reduceByKey(lambda a, b: [a[i] + b[i] for i in range(len(a))]).collect()\n",
    "    with open(embOutputPath, 'w') as f:\n",
    "        for row in result:\n",
    "            vectors = \" \".join([str(emb) for emb in row[1]])\n",
    "            f.write(row[0] + \":\" + vectors + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b384731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c2d2efb34b10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membLength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mratingSamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawSampleDataPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msortUdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUdfFunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "embLength = 10\n",
    "ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "sortUdf = udf(UdfFunction.sortF, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6413ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSeq = ratingSamples.where(F.col(\"rating\") >= 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSeq.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a81429",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[1,5,7,10,23,20,6,5,10,7,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5349cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  [1,5,7,10,23,20,6,5,10,7,10]\n",
    "rdd_data = sc.parallelize(data)\n",
    "s = rdd_data.reduce(lambda x,y:x+y+0.0)\n",
    "n = rdd_data.count()\n",
    "avg = s/n\n",
    "print(\"average\", avg)\n",
    "print(\"count\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_count = rdd_data.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd25e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [(\"LiLei\",18,87),(\"HanMeiMei\",16,77),(\"DaChui\",16,66),(\"Jim\",18,77),(\"RuHua\",18,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  [1,5,7,10,23,20,6,5,10,7,10]\n",
    "rdd_data = sc.parallelize(data)\n",
    "rdd_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.takeSample(False, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "rdd_data.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRdd = sc.parallelize([(1,1),(1,4),(3,9),(2,16)]) \n",
    "pairRdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356bbd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda x:x>10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = sc.parallelize([(\"LiLei\",18),\n",
    "                        (\"HanMeiMei\",16)])\n",
    "gender = sc.parallelize([(\"LiLei\",\"male\"),\n",
    "                        (\"HanMeiMei\",\"female\"),(\"Lucy\",\"female\")])\n",
    "age.leftOuterJoin(gender).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af602265",
   "metadata": {},
   "outputs": [],
   "source": [
    "age.rightOuterJoin(gender).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,5,7,10,23,20,6,5,10,7,10]\n",
    "data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave = data.reduce(lambda x, y: x + y)/data.count()\n",
    "print(ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb706916",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = data.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = data_count.map(lambda x:x[1]).reduce(lambda x, y: x if x >= y else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_mode = rdd_count.filter(lambda x:x[1]==max_count).map(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_mode.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad206e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_mode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [(\"LiLei\",18,87),(\"HanMeiMei\",16,77),(\"DaChui\",16,66),(\"Jim\",18,77),(\"RuHua\",18,50)]\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "students =  sc.parallelize(students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "students.sortBy(lambda x:x[2], ascending=False).map(lambda x: x[0]).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,7,8,5,3,18,34,9,0,12,8]\n",
    "data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f55d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sortBy(lambda x: x).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "students.sortBy(lambda x: (x[2], x[1]), ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [(\"class1\",\"LiLei\"), (\"class1\",\"HanMeiMei\"),(\"class2\",\"DaChui\"),(\"class2\",\"RuHua\")]\n",
    "scores = [(\"LiLei\",76),(\"HanMeiMei\",80),(\"DaChui\",70),(\"RuHua\",60)]\n",
    "rdd_classes = sc.parallelize(classes).map(lambda x: (x[1], x[0]))\n",
    "rdd_scores = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322638a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_join = rdd_scores.join(rdd_classes).map(lambda x:(x[1][1], x[1][0]))\n",
    "rdd_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(iterator):\n",
    "    data = list(iterator)\n",
    "    s = 0.0\n",
    "    for x in data:\n",
    "        s = s + x\n",
    "    return s/len(data)\n",
    "\n",
    "rdd_join.groupByKey().map(lambda x:(x[0], average(x[1]))).filter(lambda t:t[1]>75).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74a36a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-SCD57NT executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-fda7637df145>\", line 16, in <lambda>\n  File \"<ipython-input-8-fda7637df145>\", line 7, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-fda7637df145>\", line 16, in <lambda>\n  File \"<ipython-input-8-fda7637df145>\", line 7, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fda7637df145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mrdd_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd_classes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd_mode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \"\"\"\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-SCD57NT executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-fda7637df145>\", line 16, in <lambda>\n  File \"<ipython-input-8-fda7637df145>\", line 7, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-fda7637df145>\", line 16, in <lambda>\n  File \"<ipython-input-8-fda7637df145>\", line 7, in mode\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 163, in max\n    return _invoke_function_over_column(\"max\", col)\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 66, in _invoke_function_over_column\n    return _invoke_function(name, _to_java_column(col))\n  File \"C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 45, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: dict_values([2, 1]) of type <class 'dict_values'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "students = [(\"class1\",15),(\"class1\",15),(\"class2\",16),(\"class2\",16),(\"class1\",17),(\"class2\",19)]\n",
    "def mode(arr):\n",
    "    dict_cnt = {}\n",
    "    for x in arr:\n",
    "        dict_cnt[x] = dict_cnt.get(x,0)+1\n",
    "    max_cnt = max(dict_cnt.values())\n",
    "    most_values = [k for k,v in dict_cnt.items() if v==max_cnt]\n",
    "    s = 0.0\n",
    "    for x in most_values:\n",
    "        s = s + x\n",
    "    return s/len(most_values)\n",
    "\n",
    "rdd_students = sc.parallelize(students)\n",
    "rdd_classes = rdd_students.aggregateByKey([],lambda arr,x:arr+[x],lambda arr1,arr2:arr1+arr2)\n",
    "rdd_mode = rdd_classes.map(lambda t:(t[0],mode(t[1])))\n",
    "\n",
    "print(rdd_mode.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [(\"class1\",15),(\"class1\",15),(\"class2\",16),(\"class2\",16),(\"class1\",17),(\"class2\",19)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ead1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6752a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
